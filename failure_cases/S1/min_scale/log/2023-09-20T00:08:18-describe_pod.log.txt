idx,0,LOG_TIME,2023-09-20T05:08:18,2023-09-20T00:08:18
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m36s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m35s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m28s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m27s  kubelet            Created container metrics-server
  Normal  Started    9m27s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,1,LOG_TIME,2023-09-20T05:08:19,2023-09-20T00:08:19
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m37s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m36s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m29s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m28s  kubelet            Created container metrics-server
  Normal  Started    9m28s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,2,LOG_TIME,2023-09-20T05:08:20,2023-09-20T00:08:20
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m38s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m37s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m30s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m29s  kubelet            Created container metrics-server
  Normal  Started    9m29s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,3,LOG_TIME,2023-09-20T05:08:21,2023-09-20T00:08:21
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m39s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m38s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m31s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m30s  kubelet            Created container metrics-server
  Normal  Started    9m30s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,4,LOG_TIME,2023-09-20T05:08:22,2023-09-20T00:08:22
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m40s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m39s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m32s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m31s  kubelet            Created container metrics-server
  Normal  Started    9m31s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,5,LOG_TIME,2023-09-20T05:08:23,2023-09-20T00:08:23
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     11m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 11m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m41s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m40s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m33s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m32s  kubelet            Created container metrics-server
  Normal  Started    9m32s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,6,LOG_TIME,2023-09-20T05:08:24,2023-09-20T00:08:24
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     11m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m42s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m41s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m34s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m33s  kubelet            Created container metrics-server
  Normal  Started    9m33s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,7,LOG_TIME,2023-09-20T05:08:26,2023-09-20T00:08:26
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  0s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m43s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m42s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m35s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m34s  kubelet            Created container metrics-server
  Normal  Started    9m34s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,8,LOG_TIME,2023-09-20T05:08:27,2023-09-20T00:08:27
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m44s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m43s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m36s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m35s  kubelet            Created container metrics-server
  Normal  Started    9m35s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,9,LOG_TIME,2023-09-20T05:08:28,2023-09-20T00:08:28
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  2s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     1s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    1s    kubelet            Created container descheduler
  Normal  Started    0s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     11m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m45s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m44s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m37s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m36s  kubelet            Created container metrics-server
  Normal  Started    9m36s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,10,LOG_TIME,2023-09-20T05:08:29,2023-09-20T00:08:29
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  3s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     2s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    2s    kubelet            Created container descheduler
  Normal  Started    1s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     11m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m46s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m45s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m38s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m37s  kubelet            Created container metrics-server
  Normal  Started    9m37s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,11,LOG_TIME,2023-09-20T05:08:30,2023-09-20T00:08:30
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  4s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     3s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    3s    kubelet            Created container descheduler
  Normal  Started    2s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m47s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m46s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m39s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m38s  kubelet            Created container metrics-server
  Normal  Started    9m38s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,12,LOG_TIME,2023-09-20T05:08:31,2023-09-20T00:08:31
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  5s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     4s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    4s    kubelet            Created container descheduler
  Normal  Started    3s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    11m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m48s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m47s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m40s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m39s  kubelet            Created container metrics-server
  Normal  Started    9m39s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,13,LOG_TIME,2023-09-20T05:08:32,2023-09-20T00:08:32
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     5s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    5s    kubelet            Created container descheduler
  Normal  Started    4s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    11m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m49s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m48s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m41s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m40s  kubelet            Created container metrics-server
  Normal  Started    9m40s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,14,LOG_TIME,2023-09-20T05:08:33,2023-09-20T00:08:33
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  7s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     6s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    6s    kubelet            Created container descheduler
  Normal  Started    5s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m51s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m50s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m43s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m42s  kubelet            Created container metrics-server
  Normal  Started    9m42s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,15,LOG_TIME,2023-09-20T05:08:34,2023-09-20T00:08:34
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  8s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     7s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    7s    kubelet            Created container descheduler
  Normal  Started    6s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 11m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m52s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m51s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m44s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m43s  kubelet            Created container metrics-server
  Normal  Started    9m43s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,16,LOG_TIME,2023-09-20T05:08:35,2023-09-20T00:08:35
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  9s    default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     8s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    8s    kubelet            Created container descheduler
  Normal  Started    7s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m53s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m52s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m45s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m44s  kubelet            Created container metrics-server
  Normal  Started    9m44s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,17,LOG_TIME,2023-09-20T05:08:36,2023-09-20T00:08:36
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11s   default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     10s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    10s   kubelet            Created container descheduler
  Normal  Started    9s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m54s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m53s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m46s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m45s  kubelet            Created container metrics-server
  Normal  Started    9m45s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,18,LOG_TIME,2023-09-20T05:08:37,2023-09-20T00:08:37
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12s   default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     11s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    11s   kubelet            Created container descheduler
  Normal  Started    10s   kubelet            Started container descheduler


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    11m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m55s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m54s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m47s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m46s  kubelet            Created container metrics-server
  Normal  Started    9m46s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,19,LOG_TIME,2023-09-20T05:08:38,2023-09-20T00:08:38
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-6bbbff857b-frmh7
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Wed, 20 Sep 2023 00:08:26 -0500
Labels:               app=descheduler
                      pod-template-hash=6bbbff857b
Annotations:          <none>
Status:               Running
IP:                   10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13s   default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     12s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    12s   kubelet            Created container descheduler
  Normal  Started    11s   kubelet            Started container descheduler


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  2s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     1s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    11m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    11m   kubelet            Created container kube-proxy
  Normal  Started    11m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m56s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m55s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m48s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m47s  kubelet            Created container metrics-server
  Normal  Started    9m47s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,20,LOG_TIME,2023-09-20T05:08:39,2023-09-20T00:08:39
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                      descheduler-6bbbff857b-frmh7
Namespace:                 kube-system
Priority:                  2000000000
Priority Class Name:       system-cluster-critical
Node:                      kind-worker2/172.18.0.4
Start Time:                Wed, 20 Sep 2023 00:08:26 -0500
Labels:                    app=descheduler
                           pod-template-hash=6bbbff857b
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        10.244.1.6
IPs:
  IP:           10.244.1.6
Controlled By:  ReplicaSet/descheduler-6bbbff857b
Containers:
  descheduler:
    Container ID:  containerd://fd9df42acba9316f804eaa90f7ee0105c4fe3e1fe5346be162492dfa94d768d3
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lt6rz (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-lt6rz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  15s   default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     14s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    14s   kubelet            Created container descheduler
  Normal  Started    13s   kubelet            Started container descheduler
  Normal  Killing    1s    kubelet            Stopping container descheduler


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  4s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     3s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    2s    kubelet            Created container descheduler
  Normal  Started    2s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  9m59s  default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m58s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m51s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m50s  kubelet            Created container metrics-server
  Normal  Started    9m50s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,21,LOG_TIME,2023-09-20T05:08:42,2023-09-20T00:08:42
Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Pod 'descheduler-6bbbff857b-frmh7': error 'pods "descheduler-6bbbff857b-frmh7" not found', but found events.
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  16s   default-scheduler  Successfully assigned kube-system/descheduler-6bbbff857b-frmh7 to kind-worker2
  Normal  Pulled     15s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    15s   kubelet            Created container descheduler
  Normal  Started    14s   kubelet            Started container descheduler
  Normal  Killing    2s    kubelet            Stopping container descheduler


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  5s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     4s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    3s    kubelet            Created container descheduler
  Normal  Started    3s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    9m59s  kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m52s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m51s  kubelet            Created container metrics-server
  Normal  Started    9m51s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,22,LOG_TIME,2023-09-20T05:08:43,2023-09-20T00:08:43
Name:           background-1-8449dbdffd-54vvv
Namespace:      default
Priority:       0
Node:           kind-worker2/172.18.0.4
Start Time:     Wed, 20 Sep 2023 00:08:43 -0500
Labels:         pod-template-hash=8449dbdffd
                run=background-1
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     5s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    4s    kubelet            Created container descheduler
  Normal  Started    4s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m53s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m52s  kubelet            Created container metrics-server
  Normal  Started    9m52s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,23,LOG_TIME,2023-09-20T05:08:44,2023-09-20T00:08:44
Name:           background-1-8449dbdffd-54vvv
Namespace:      default
Priority:       0
Node:           kind-worker2/172.18.0.4
Start Time:     Wed, 20 Sep 2023 00:08:43 -0500
Labels:         pod-template-hash=8449dbdffd
                run=background-1
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  0s    kubelet  Container image "nginx:1.14.2" already present on machine


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  7s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     6s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    5s    kubelet            Created container descheduler
  Normal  Started    5s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m54s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m53s  kubelet            Created container metrics-server
  Normal  Started    9m53s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,24,LOG_TIME,2023-09-20T05:08:45,2023-09-20T00:08:45
Name:           background-1-8449dbdffd-54vvv
Namespace:      default
Priority:       0
Node:           kind-worker2/172.18.0.4
Start Time:     Wed, 20 Sep 2023 00:08:43 -0500
Labels:         pod-template-hash=8449dbdffd
                run=background-1
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   1s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  0s    kubelet  Created container nginx
  Normal  Started  0s    kubelet  Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  8s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     7s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    6s    kubelet            Created container descheduler
  Normal  Started    6s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m55s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m54s  kubelet            Created container metrics-server
  Normal  Started    9m54s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,25,LOG_TIME,2023-09-20T05:08:46,2023-09-20T00:08:46
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   2s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  1s    kubelet  Created container nginx
  Normal  Started  1s    kubelet  Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  9s    default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     8s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    7s    kubelet            Created container descheduler
  Normal  Started    7s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m56s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m55s  kubelet            Created container metrics-server
  Normal  Started    9m55s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,26,LOG_TIME,2023-09-20T05:08:47,2023-09-20T00:08:47
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   3s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  2s    kubelet  Created container nginx
  Normal  Started  2s    kubelet  Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     9s    kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    8s    kubelet            Created container descheduler
  Normal  Started    8s    kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  12m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m57s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m56s  kubelet            Created container metrics-server
  Normal  Started    9m56s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,27,LOG_TIME,2023-09-20T05:08:48,2023-09-20T00:08:48
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   5s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  4s    kubelet  Created container nginx
  Normal  Started  4s    kubelet  Started container nginx


Name:           nginx-deployment-6dccd8f6b9-4vfpv
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dzrlx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-dzrlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker


Name:           nginx-deployment-6dccd8f6b9-82pss
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fs66c (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-fs66c:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-82pss to kind-worker


Name:           nginx-deployment-6dccd8f6b9-dx5ww
Namespace:      default
Priority:       0
Node:           kind-worker2/172.18.0.4
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2


Name:           nginx-deployment-6dccd8f6b9-x9jsk
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:49 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  0s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     12s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    11s   kubelet            Created container descheduler
  Normal  Started    11s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  10m    default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m    kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     9m59s  kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    9m58s  kubelet            Created container metrics-server
  Normal  Started    9m58s  kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,28,LOG_TIME,2023-09-20T05:08:50,2023-09-20T00:08:50
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   6s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  5s    kubelet  Created container nginx
  Normal  Started  5s    kubelet  Started container nginx


Name:           nginx-deployment-6dccd8f6b9-4vfpv
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dzrlx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-dzrlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker


Name:           nginx-deployment-6dccd8f6b9-82pss
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fs66c (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-fs66c:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-82pss to kind-worker
  Normal  Pulled     0s    kubelet            Container image "nginx:1.14.2" already present on machine


Name:           nginx-deployment-6dccd8f6b9-dx5ww
Namespace:      default
Priority:       0
Node:           kind-worker2/172.18.0.4
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     1s    kubelet            Container image "nginx:1.14.2" already present on machine


Name:           nginx-deployment-6dccd8f6b9-x9jsk
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:49 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  2s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  15s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     14s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    13s   kubelet            Created container descheduler
  Normal  Started    13s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,29,LOG_TIME,2023-09-20T05:08:53,2023-09-20T00:08:53
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   9s    kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  8s    kubelet  Created container nginx
  Normal  Started  8s    kubelet  Started container nginx


Name:           nginx-deployment-6dccd8f6b9-4nq2k
Namespace:      default
Priority:       0
Node:           <none>
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Image:      nginx:1.14.2
    Port:       80/TCP
    Host Port:  0/TCP
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  3s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.


Name:           nginx-deployment-6dccd8f6b9-4vfpv
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:48 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dzrlx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-dzrlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  6s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker
  Normal  Pulled     3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    1s    kubelet            Created container nginx
  Normal  Started    0s    kubelet            Started container nginx


Name:                      nginx-deployment-6dccd8f6b9-82pss
Namespace:                 default
Priority:                  0
Node:                      kind-worker/172.18.0.2
Start Time:                Wed, 20 Sep 2023 00:08:48 -0500
Labels:                    pod-template-hash=6dccd8f6b9
                           run=nginx
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        10.244.2.15
IPs:
  IP:           10.244.2.15
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://e463fdf22d9618a3c4f353e612ed4ec329e3e9aba0c8b9daa864819386dd048f
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:53 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fs66c (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fs66c:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason            Age   From                     Message
  ----    ------            ----  ----                     -------
  Normal  RemoveDuplicates  4s    sigs.k8s.io.descheduler  pod evicted by sigs.k8s.io/descheduler
  Normal  Scheduled         7s    default-scheduler        Successfully assigned default/nginx-deployment-6dccd8f6b9-82pss to kind-worker
  Normal  Pulled            4s    kubelet                  Container image "nginx:1.14.2" already present on machine
  Normal  Created           2s    kubelet                  Created container nginx
  Normal  Started           2s    kubelet                  Started container nginx
  Normal  Killing           1s    kubelet                  Stopping container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  8s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     6s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    4s    kubelet            Created container nginx
  Normal  Started    4s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  8s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     6s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    4s    kubelet            Created container nginx
  Normal  Started    2s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  20s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     19s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    18s   kubelet            Created container descheduler
  Normal  Started    18s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    12m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  12m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,30,LOG_TIME,2023-09-20T05:08:58,2023-09-20T00:08:58
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  13s   kubelet  Created container nginx
  Normal  Started  13s   kubelet  Started container nginx


Name:           nginx-deployment-6dccd8f6b9-4nq2k
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:57 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker


Name:         nginx-deployment-6dccd8f6b9-4vfpv
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.16
IPs:
  IP:           10.244.2.16
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://32dba443e4e4135c97c63cfbceb04337ec85246a534656de35ca5817b4c884ce
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:53 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dzrlx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-dzrlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker
  Normal  Pulled     7s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    5s    kubelet            Created container nginx
  Normal  Started    4s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     8s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    6s    kubelet            Created container nginx
  Normal  Started    6s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  9s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     7s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    5s    kubelet            Created container nginx
  Normal  Started    3s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  21s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     20s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    19s   kubelet            Created container descheduler
  Normal  Started    19s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,31,LOG_TIME,2023-09-20T05:08:59,2023-09-20T00:08:59
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   16s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  15s   kubelet  Created container nginx
  Normal  Started  15s   kubelet  Started container nginx


Name:           nginx-deployment-6dccd8f6b9-4nq2k
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:08:57 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            1s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           0s    kubelet            Created container nginx


Name:         nginx-deployment-6dccd8f6b9-4vfpv
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.16
IPs:
  IP:           10.244.2.16
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://32dba443e4e4135c97c63cfbceb04337ec85246a534656de35ca5817b4c884ce
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:53 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dzrlx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-dzrlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker
  Normal  Pulled     9s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    7s    kubelet            Created container nginx
  Normal  Started    6s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     10s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    8s    kubelet            Created container nginx
  Normal  Started    8s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     9s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    7s    kubelet            Created container nginx
  Normal  Started    5s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  23s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     22s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    21s   kubelet            Created container descheduler
  Normal  Started    21s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      12m   kubelet            Created container kindnet-cni
  Normal   Started      12m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  11m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,32,LOG_TIME,2023-09-20T05:09:01,2023-09-20T00:09:01
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   21s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  20s   kubelet  Created container nginx
  Normal  Started  20s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  14s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         8s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            6s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           5s    kubelet            Created container nginx
  Normal   Started           5s    kubelet            Started container nginx


Pod 'nginx-deployment-6dccd8f6b9-4vfpv': error 'pods "nginx-deployment-6dccd8f6b9-4vfpv" not found', but found events.
Events:
  Type    Reason            Age   From                     Message
  ----    ------            ----  ----                     -------
  Normal  RemoveDuplicates  4s    sigs.k8s.io.descheduler  pod evicted by sigs.k8s.io/descheduler
  Normal  Scheduled         17s   default-scheduler        Successfully assigned default/nginx-deployment-6dccd8f6b9-4vfpv to kind-worker
  Normal  Pulled            14s   kubelet                  Container image "nginx:1.14.2" already present on machine
  Normal  Created           12s   kubelet                  Created container nginx
  Normal  Started           11s   kubelet                  Started container nginx
  Normal  Killing           4s    kubelet                  Stopping container nginx


Name:           nginx-deployment-6dccd8f6b9-9nwvn
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:05 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbbs5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-wbbs5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-9nwvn to kind-worker


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  18s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     16s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    14s   kubelet            Created container nginx
  Normal  Started    14s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  17s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     15s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    13s   kubelet            Created container nginx
  Normal  Started    11s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  29s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     28s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    27s   kubelet            Created container descheduler
  Normal  Started    27s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,33,LOG_TIME,2023-09-20T05:09:07,2023-09-20T00:09:07
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   23s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  22s   kubelet  Created container nginx
  Normal  Started  22s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         11s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            9s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           8s    kubelet            Created container nginx
  Normal   Started           8s    kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-9nwvn
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:05 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbbs5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-wbbs5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-9nwvn to kind-worker


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  20s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     18s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    16s   kubelet            Created container nginx
  Normal  Started    16s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  19s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     17s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    15s   kubelet            Created container nginx
  Normal  Started    13s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  31s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     30s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    29s   kubelet            Created container descheduler
  Normal  Started    29s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         12m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,34,LOG_TIME,2023-09-20T05:09:08,2023-09-20T00:09:08
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   24s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  23s   kubelet  Created container nginx
  Normal  Started  23s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         11s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            9s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           8s    kubelet            Created container nginx
  Normal   Started           8s    kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-9nwvn
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:05 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbbs5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-wbbs5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-9nwvn to kind-worker
  Normal   Pulled            0s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           0s    kubelet            Created container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  20s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     18s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    16s   kubelet            Created container nginx
  Normal  Started    16s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  20s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     18s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    16s   kubelet            Created container nginx
  Normal  Started    14s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  32s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     31s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    30s   kubelet            Created container descheduler
  Normal  Started    30s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     13m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   13m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           12m   kubelet            Created container local-path-provisioner
  Normal   Started           12m   kubelet            Started container local-path-provisioner
idx,35,LOG_TIME,2023-09-20T05:09:10,2023-09-20T00:09:10
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   26s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  25s   kubelet  Created container nginx
  Normal  Started  25s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  20s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         14s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            12s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           11s   kubelet            Created container nginx
  Normal   Started           11s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-9nwvn
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:05 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.19
IPs:
  IP:           10.244.2.19
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://78c2f6f9efca2281aa8c70434a196fb8f390576c53f29d37dd84898eff7e24e8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:09 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wbbs5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-wbbs5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  10s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         6s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-9nwvn to kind-worker
  Normal   Pulled            3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           3s    kubelet            Created container nginx
  Normal   Started           2s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  23s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     21s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    19s   kubelet            Created container nginx
  Normal  Started    19s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  22s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     20s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    18s   kubelet            Created container nginx
  Normal  Started    16s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            12m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           12m   kubelet            Created container coredns
  Normal   Started           12m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  34s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     33s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    32s   kubelet            Created container descheduler
  Normal  Started    32s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  13m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,36,LOG_TIME,2023-09-20T05:09:14,2023-09-20T00:09:14
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   31s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  30s   kubelet  Created container nginx
  Normal  Started  30s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  24s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         18s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            16s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           15s   kubelet            Created container nginx
  Normal   Started           15s   kubelet            Started container nginx


Pod 'nginx-deployment-6dccd8f6b9-9nwvn': error 'pods "nginx-deployment-6dccd8f6b9-9nwvn" not found', but found events.
Events:
  Type     Reason            Age   From                     Message
  ----     ------            ----  ----                     -------
  Normal   RemoveDuplicates  4s    sigs.k8s.io.descheduler  pod evicted by sigs.k8s.io/descheduler
  Warning  FailedScheduling  15s   default-scheduler        0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         11s   default-scheduler        Successfully assigned default/nginx-deployment-6dccd8f6b9-9nwvn to kind-worker
  Normal   Pulled            8s    kubelet                  Container image "nginx:1.14.2" already present on machine
  Normal   Created           8s    kubelet                  Created container nginx
  Normal   Started           7s    kubelet                  Started container nginx
  Normal   Killing           5s    kubelet                  Stopping container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  28s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     26s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    24s   kubelet            Created container nginx
  Normal  Started    24s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-q6s5c
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:15 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  27s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     25s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    23s   kubelet            Created container nginx
  Normal  Started    21s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  39s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     38s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    37s   kubelet            Created container descheduler
  Normal  Started    37s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    13m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  13m   kubelet  Created container kube-controller-manager
  Normal  Started  13m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  13m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,37,LOG_TIME,2023-09-20T05:09:16,2023-09-20T00:09:16
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   33s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  32s   kubelet  Created container nginx
  Normal  Started  32s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  26s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         20s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            18s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           17s   kubelet            Created container nginx
  Normal   Started           17s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  29s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     27s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    25s   kubelet            Created container nginx
  Normal  Started    25s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-q6s5c
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:15 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  6s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         2s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  28s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     26s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    24s   kubelet            Created container nginx
  Normal  Started    22s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  40s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     39s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    38s   kubelet            Created container descheduler
  Normal  Started    38s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    13m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,38,LOG_TIME,2023-09-20T05:09:18,2023-09-20T00:09:18
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   34s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  33s   kubelet  Created container nginx
  Normal  Started  33s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  27s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         21s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            19s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           18s   kubelet            Created container nginx
  Normal   Started           18s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  30s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     28s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    26s   kubelet            Created container nginx
  Normal  Started    26s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-q6s5c
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:15 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            1s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           0s    kubelet            Created container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  29s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     27s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    25s   kubelet            Created container nginx
  Normal  Started    23s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     40s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    39s   kubelet            Created container descheduler
  Normal  Started    39s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  12m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,39,LOG_TIME,2023-09-20T05:09:19,2023-09-20T00:09:19
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   35s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  34s   kubelet  Created container nginx
  Normal  Started  34s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  28s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         22s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            20s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           19s   kubelet            Created container nginx
  Normal   Started           19s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  31s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     29s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    27s   kubelet            Created container nginx
  Normal  Started    27s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  8s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         4s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            2s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           1s    kubelet            Created container nginx
  Normal   Started           1s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  30s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     28s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    26s   kubelet            Created container nginx
  Normal  Started    24s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     41s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    40s   kubelet            Created container descheduler
  Normal  Started    40s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  12m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,40,LOG_TIME,2023-09-20T05:09:20,2023-09-20T00:09:20
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   36s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  35s   kubelet  Created container nginx
  Normal  Started  35s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  29s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         23s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            21s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           20s   kubelet            Created container nginx
  Normal   Started           20s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  32s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     30s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    28s   kubelet            Created container nginx
  Normal  Started    28s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         5s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           2s    kubelet            Created container nginx
  Normal   Started           2s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-x9jsk
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:49 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.17
IPs:
  IP:           10.244.2.17
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://7764c7e3da8c1c064ef5852c7b355b32d17c0baad58d253502ab93d1d4d5b0c8
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:54 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cxmwg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-cxmwg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  31s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-x9jsk to kind-worker
  Normal  Pulled     29s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    27s   kubelet            Created container nginx
  Normal  Started    25s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  44s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     43s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    42s   kubelet            Created container descheduler
  Normal  Started    42s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,41,LOG_TIME,2023-09-20T05:09:23,2023-09-20T00:09:23
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   40s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  39s   kubelet  Created container nginx
  Normal  Started  39s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  33s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         27s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            25s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           24s   kubelet            Created container nginx
  Normal   Started           24s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-b9fns
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:23 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  3s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  36s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     34s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    32s   kubelet            Created container nginx
  Normal  Started    32s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         9s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            7s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           6s    kubelet            Created container nginx
  Normal   Started           6s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  47s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     46s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    45s   kubelet            Created container descheduler
  Normal  Started    45s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     12m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     12m (x2 over 12m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,42,LOG_TIME,2023-09-20T05:09:25,2023-09-20T00:09:25
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   41s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  40s   kubelet  Created container nginx
  Normal  Started  40s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  34s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         28s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            26s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           25s   kubelet            Created container nginx
  Normal   Started           25s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-b9fns
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:23 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  4s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         2s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            0s    kubelet            Container image "nginx:1.14.2" already present on machine


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  37s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     35s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    33s   kubelet            Created container nginx
  Normal  Started    33s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  14s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         10s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            8s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           7s    kubelet            Created container nginx
  Normal   Started           7s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  48s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     47s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    46s   kubelet            Created container descheduler
  Normal  Started    46s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,43,LOG_TIME,2023-09-20T05:09:26,2023-09-20T00:09:26
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   43s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  42s   kubelet  Created container nginx
  Normal  Started  42s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  36s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         30s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            28s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           27s   kubelet            Created container nginx
  Normal   Started           27s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-b9fns
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:23 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  6s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         4s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            2s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           1s    kubelet            Created container nginx
  Normal   Started           1s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  39s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     37s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    35s   kubelet            Created container nginx
  Normal  Started    35s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  16s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         12s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            10s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           9s    kubelet            Created container nginx
  Normal   Started           9s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  50s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     49s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    48s   kubelet            Created container descheduler
  Normal  Started    48s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     12m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,44,LOG_TIME,2023-09-20T05:09:28,2023-09-20T00:09:28
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   44s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  43s   kubelet  Created container nginx
  Normal  Started  43s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  37s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         31s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            29s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           28s   kubelet            Created container nginx
  Normal   Started           28s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         5s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           2s    kubelet            Created container nginx
  Normal   Started           2s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  40s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     38s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    36s   kubelet            Created container nginx
  Normal  Started    36s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         13s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            11s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           10s   kubelet            Created container nginx
  Normal   Started           10s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  51s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     50s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    49s   kubelet            Created container descheduler
  Normal  Started    49s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,45,LOG_TIME,2023-09-20T05:09:29,2023-09-20T00:09:29
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   45s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  44s   kubelet  Created container nginx
  Normal  Started  44s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  38s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         32s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            30s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           29s   kubelet            Created container nginx
  Normal   Started           29s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  8s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         6s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            4s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           3s    kubelet            Created container nginx
  Normal   Started           3s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     39s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    37s   kubelet            Created container nginx
  Normal  Started    37s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  18s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         14s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            12s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           11s   kubelet            Created container nginx
  Normal   Started           11s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     51s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    50s   kubelet            Created container descheduler
  Normal  Started    50s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     12m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,46,LOG_TIME,2023-09-20T05:09:30,2023-09-20T00:09:30
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   46s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  45s   kubelet  Created container nginx
  Normal  Started  45s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  39s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         33s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            31s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           30s   kubelet            Created container nginx
  Normal   Started           30s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         7s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            5s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           4s    kubelet            Created container nginx
  Normal   Started           4s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     40s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    38s   kubelet            Created container nginx
  Normal  Started    38s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-q6s5c
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:15 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  19s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         15s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            13s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           12s   kubelet            Created container nginx
  Normal   Started           12s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     52s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    51s   kubelet            Created container descheduler
  Normal  Started    51s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    12m                kubelet            Created container kube-proxy
  Normal   Started    12m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,47,LOG_TIME,2023-09-20T05:09:31,2023-09-20T00:09:31
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   48s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  47s   kubelet  Created container nginx
  Normal  Started  47s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  41s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         35s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            33s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           32s   kubelet            Created container nginx
  Normal   Started           32s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  11s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         9s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            7s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           6s    kubelet            Created container nginx
  Normal   Started           6s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  44s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     42s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    40s   kubelet            Created container nginx
  Normal  Started    40s   kubelet            Started container nginx


Name:                      nginx-deployment-6dccd8f6b9-q6s5c
Namespace:                 default
Priority:                  0
Node:                      kind-worker/172.18.0.2
Start Time:                Wed, 20 Sep 2023 00:09:15 -0500
Labels:                    pod-template-hash=6dccd8f6b9
                           run=nginx
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        10.244.2.20
IPs:
  IP:           10.244.2.20
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://9897794e70c3e0b204b2dcc2640fa3ac938207d4b2bfc0cbef56529bf649f1f1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:18 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fnrc2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-fnrc2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From                     Message
  ----     ------            ----  ----                     -------
  Normal   RemoveDuplicates  1s    sigs.k8s.io.descheduler  pod evicted by sigs.k8s.io/descheduler
  Warning  FailedScheduling  22s   default-scheduler        0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         18s   default-scheduler        Successfully assigned default/nginx-deployment-6dccd8f6b9-q6s5c to kind-worker
  Normal   Pulled            16s   kubelet                  Container image "nginx:1.14.2" already present on machine
  Normal   Created           15s   kubelet                  Created container nginx
  Normal   Started           15s   kubelet                  Started container nginx
  Normal   Killing           2s    kubelet                  Stopping container nginx


Name:           nginx-deployment-6dccd8f6b9-wv6cq
Namespace:      default
Priority:       0
Node:           <none>
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Image:      nginx:1.14.2
    Port:       80/TCP
    Host Port:  0/TCP
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  56s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     55s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    54s   kubelet            Created container descheduler
  Normal  Started    54s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,48,LOG_TIME,2023-09-20T05:09:34,2023-09-20T00:09:34
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   50s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  49s   kubelet  Created container nginx
  Normal  Started  49s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  43s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         37s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            35s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           34s   kubelet            Created container nginx
  Normal   Started           34s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         11s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            9s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           8s    kubelet            Created container nginx
  Normal   Started           8s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  47s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     45s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    43s   kubelet            Created container nginx
  Normal  Started    43s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-wv6cq
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:33 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  4s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         2s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  58s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     57s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    56s   kubelet            Created container descheduler
  Normal  Started    56s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 12m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,49,LOG_TIME,2023-09-20T05:09:36,2023-09-20T00:09:36
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   52s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  51s   kubelet  Created container nginx
  Normal  Started  51s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  45s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         39s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            37s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           36s   kubelet            Created container nginx
  Normal   Started           36s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  15s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         13s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            11s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           10s   kubelet            Created container nginx
  Normal   Started           10s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  48s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     46s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    44s   kubelet            Created container nginx
  Normal  Started    44s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-wv6cq
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:33 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            0s    kubelet            Container image "nginx:1.14.2" already present on machine


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  59s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     58s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    57s   kubelet            Created container descheduler
  Normal  Started    57s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,50,LOG_TIME,2023-09-20T05:09:37,2023-09-20T00:09:37
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   53s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  52s   kubelet  Created container nginx
  Normal  Started  52s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  47s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         41s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            39s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           38s   kubelet            Created container nginx
  Normal   Started           38s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         15s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            13s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           12s   kubelet            Created container nginx
  Normal   Started           12s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  50s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     48s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    46s   kubelet            Created container nginx
  Normal  Started    46s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         5s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            2s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           2s    kubelet            Created container nginx
  Normal   Started           1s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  61s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     60s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    59s   kubelet            Created container descheduler
  Normal  Started    59s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    12m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,51,LOG_TIME,2023-09-20T05:09:38,2023-09-20T00:09:38
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   54s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  53s   kubelet  Created container nginx
  Normal  Started  53s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  47s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         41s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            39s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           38s   kubelet            Created container nginx
  Normal   Started           38s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         15s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            13s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           12s   kubelet            Created container nginx
  Normal   Started           12s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  51s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     49s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    47s   kubelet            Created container nginx
  Normal  Started    47s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  8s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         6s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           3s    kubelet            Created container nginx
  Normal   Started           2s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  62s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     61s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    60s   kubelet            Created container descheduler
  Normal  Started    60s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    12m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    12m   kubelet            Created container kube-proxy
  Normal  Started    12m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,52,LOG_TIME,2023-09-20T05:09:39,2023-09-20T00:09:39
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   55s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  54s   kubelet  Created container nginx
  Normal  Started  54s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  49s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         43s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            41s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           40s   kubelet            Created container nginx
  Normal   Started           40s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  19s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         17s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            15s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           14s   kubelet            Created container nginx
  Normal   Started           14s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  52s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     50s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    48s   kubelet            Created container nginx
  Normal  Started    48s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         7s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            4s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           4s    kubelet            Created container nginx
  Normal   Started           3s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  63s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     62s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    61s   kubelet            Created container descheduler
  Normal  Started    61s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m                kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    10m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,53,LOG_TIME,2023-09-20T05:09:40,2023-09-20T00:09:40
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   56s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  55s   kubelet  Created container nginx
  Normal  Started  55s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  49s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         43s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            41s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           40s   kubelet            Created container nginx
  Normal   Started           40s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-b9fns
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:23 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.21
IPs:
  IP:           10.244.2.21
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://3c10b3ee0fe02df1285fbbb5b00304292d566b633dfe3c7296fdd978a1013fda
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:26 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vmcbq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vmcbq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  19s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         17s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-b9fns to kind-worker
  Normal   Pulled            15s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           14s   kubelet            Created container nginx
  Normal   Started           14s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     51s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    49s   kubelet            Created container nginx
  Normal  Started    49s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         11s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            8s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           8s    kubelet            Created container nginx
  Normal   Started           7s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  68s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     67s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    66s   kubelet            Created container descheduler
  Normal  Started    66s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  2s (x2 over 12m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,54,LOG_TIME,2023-09-20T05:09:45,2023-09-20T00:09:45
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   62s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  61s   kubelet  Created container nginx
  Normal  Started  61s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  55s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         49s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            47s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           46s   kubelet            Created container nginx
  Normal   Started           46s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  58s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     56s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    54s   kubelet            Created container nginx
  Normal  Started    54s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-fp449
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:45 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcpn7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-tcpn7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  3s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         1s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-fp449 to kind-worker


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  15s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         13s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            10s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           10s   kubelet            Created container nginx
  Normal   Started           9s    kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  69s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     68s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    67s   kubelet            Created container descheduler
  Normal  Started    67s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  3s (x2 over 12m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,55,LOG_TIME,2023-09-20T05:09:47,2023-09-20T00:09:47
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   63s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  62s   kubelet  Created container nginx
  Normal  Started  62s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  56s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         50s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            48s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           47s   kubelet            Created container nginx
  Normal   Started           47s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  59s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     57s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    55s   kubelet            Created container nginx
  Normal  Started    55s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-fp449
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:45 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcpn7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-tcpn7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  4s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         2s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-fp449 to kind-worker
  Normal   Pulled            0s    kubelet            Container image "nginx:1.14.2" already present on machine


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  16s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         14s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            11s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           11s   kubelet            Created container nginx
  Normal   Started           10s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  71s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     70s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    69s   kubelet            Created container descheduler
  Normal  Started    69s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  5s (x2 over 12m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,56,LOG_TIME,2023-09-20T05:09:48,2023-09-20T00:09:48
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   64s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  63s   kubelet  Created container nginx
  Normal  Started  63s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  57s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         51s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            49s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           48s   kubelet            Created container nginx
  Normal   Started           48s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  60s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     58s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    56s   kubelet            Created container nginx
  Normal  Started    56s   kubelet            Started container nginx


Name:           nginx-deployment-6dccd8f6b9-fp449
Namespace:      default
Priority:       0
Node:           kind-worker/172.18.0.2
Start Time:     Wed, 20 Sep 2023 00:09:45 -0500
Labels:         pod-template-hash=6dccd8f6b9
                run=nginx
Annotations:    <none>
Status:         Pending
IP:             
IPs:            <none>
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   
    Image:          nginx:1.14.2
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcpn7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-tcpn7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         3s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-fp449 to kind-worker
  Normal   Pulled            1s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           0s    kubelet            Created container nginx
  Normal   Started           0s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  17s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         15s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            12s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           12s   kubelet            Created container nginx
  Normal   Started           11s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  72s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     71s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    70s   kubelet            Created container descheduler
  Normal  Started    70s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  13m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  6s (x2 over 12m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,57,LOG_TIME,2023-09-20T05:09:49,2023-09-20T00:09:49
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   65s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  64s   kubelet  Created container nginx
  Normal  Started  64s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  59s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         53s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            51s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           50s   kubelet            Created container nginx
  Normal   Started           50s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  62s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     60s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    58s   kubelet            Created container nginx
  Normal  Started    58s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-fp449
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:45 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.23
IPs:
  IP:           10.244.2.23
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://fea70d29712201ef836d92c036eb618fb800a72c156bf656bd7a6c2ec77a5eb1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:48 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcpn7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-tcpn7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  7s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         5s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-fp449 to kind-worker
  Normal   Pulled            3s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           2s    kubelet            Created container nginx
  Normal   Started           2s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  19s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         17s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            14s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           14s   kubelet            Created container nginx
  Normal   Started           13s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  73s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     72s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    71s   kubelet            Created container descheduler
  Normal  Started    71s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  14m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  7s (x2 over 12m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     10m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    10m   kubelet            Created container metrics-server
  Normal  Started    10m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
idx,58,LOG_TIME,2023-09-20T05:09:50,2023-09-20T00:09:50
Name:         background-1-8449dbdffd-54vvv
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:43 -0500
Labels:       pod-template-hash=8449dbdffd
              run=background-1
Annotations:  <none>
Status:       Running
IP:           10.244.1.7
IPs:
  IP:           10.244.1.7
Controlled By:  ReplicaSet/background-1-8449dbdffd
Containers:
  nginx:
    Container ID:   containerd://186caf546ab511e8de585d4753dbd028d4dd9434400e695e2bdb68e5ebe757f9
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:45 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        5
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pstjp (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-pstjp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   66s   kubelet  Container image "nginx:1.14.2" already present on machine
  Normal  Created  65s   kubelet  Created container nginx
  Normal  Started  65s   kubelet  Started container nginx


Name:         nginx-deployment-6dccd8f6b9-4nq2k
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:08:57 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.18
IPs:
  IP:           10.244.2.18
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://f94c51ff6135158581d70d8e413141ff62298114700e9756e6e378b47b252b7d
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:00 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swj68 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-swj68:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  59s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         53s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-4nq2k to kind-worker
  Normal   Pulled            51s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           50s   kubelet            Created container nginx
  Normal   Started           50s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-dx5ww
Namespace:    default
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Wed, 20 Sep 2023 00:08:48 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://1c63ab7a760e4ea8bb76e4eb6d644de54d4ba45d742c994ccc974b5439059211
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-rlbd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  63s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-dx5ww to kind-worker2
  Normal  Pulled     61s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal  Created    59s   kubelet            Created container nginx
  Normal  Started    59s   kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-fp449
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:45 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.23
IPs:
  IP:           10.244.2.23
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://fea70d29712201ef836d92c036eb618fb800a72c156bf656bd7a6c2ec77a5eb1
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:48 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcpn7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-tcpn7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  8s    default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         6s    default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-fp449 to kind-worker
  Normal   Pulled            4s    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           3s    kubelet            Created container nginx
  Normal   Started           3s    kubelet            Started container nginx


Name:         nginx-deployment-6dccd8f6b9-wv6cq
Namespace:    default
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Wed, 20 Sep 2023 00:09:33 -0500
Labels:       pod-template-hash=6dccd8f6b9
              run=nginx
Annotations:  <none>
Status:       Running
IP:           10.244.2.22
IPs:
  IP:           10.244.2.22
Controlled By:  ReplicaSet/nginx-deployment-6dccd8f6b9
Containers:
  nginx:
    Container ID:   containerd://0016f103e4a4307433e1a2734e0f518281d02ee720411dbcc0c223ade5c7bd1e
    Image:          nginx:1.14.2
    Image ID:       docker.io/library/nginx@sha256:f7988fb6c02e0ce69257d9bd9cf37ae20a60f1df7563c3a2a6abe24160306b8d
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 20 Sep 2023 00:09:37 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        2
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdcwq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-hdcwq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  20s   default-scheduler  0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   Scheduled         18s   default-scheduler  Successfully assigned default/nginx-deployment-6dccd8f6b9-wv6cq to kind-worker
  Normal   Pulled            15s   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created           15s   kubelet            Created container nginx
  Normal   Started           14s   kubelet            Started container nginx


Name:                 coredns-6d4b75cb6d-m84d9
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://23e956e5e83396bbca1aec92201b52c97c85b8e15e904ab7bf2fcc7b62a54329
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2trl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-2trl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-m84d9 to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-n8lbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:56:09 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://14c91936773f5812ad625341af2fd92e11efb4ae0bd0a235ab6d87013a5d8d5c
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:15 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5jnkl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-5jnkl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-n8lbh to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           13m   kubelet            Created container coredns
  Normal   Started           13m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-ql4ns
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Wed, 20 Sep 2023 00:08:37 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.2.14
IPs:
  IP:           10.244.2.14
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://8c6d1b6fe0a5d372ffb2e00ec457c66a3b29ac3eb82cabed10d7d58072d46845
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Wed, 20 Sep 2023 00:08:39 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8zb5m (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-8zb5m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  76s   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-ql4ns to kind-worker
  Normal  Pulled     75s   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    74s   kubelet            Created container descheduler
  Normal  Started    74s   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360905078Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://3db3356d72ab5726790d2f218b54a61ac9f8173122756a89eb439e81cd375635
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:23 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason  Age   From     Message
  ----    ------  ----  ----     -------
  Normal  Pulled  14m   kubelet  Container image "k8s.gcr.io/etcd:3.5.3-0" already present on machine


Name:         kindnet-c828x
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:56:26 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://b7b1404aa76925e1160649f75a4667b046e6f083a9f759b52fc45bcbd816d2db
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6bgdv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-6bgdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-c828x to kind-worker
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:         kindnet-nprrs
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:55:59 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://fbf60b1871fe871fa69463cb4a9938a8a2eaf3695eb45ed6fbdd00d6c139f774
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:06 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-89zmr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-89zmr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason       Age   From               Message
  ----     ------       ----  ----               -------
  Normal   Scheduled    13m   default-scheduler  Successfully assigned kube-system/kindnet-nprrs to kind-control-plane
  Warning  FailedMount  13m   kubelet            MountVolume.SetUp failed for volume "kube-api-access-89zmr" : configmap "kube-root-ca.crt" not found
  Normal   Pulled       13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal   Created      13m   kubelet            Created container kindnet-cni
  Normal   Started      13m   kubelet            Started container kindnet-cni


Name:         kindnet-t84s5
Namespace:    kube-system
Priority:     0
Node:         kind-worker2/172.18.0.4
Start Time:   Tue, 19 Sep 2023 23:56:22 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://8841a19d56c41a6b9ad897a6f9662d4edaa47b513cd4c0ff0aac0a8a2287308a
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h4tgt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-h4tgt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kindnet-t84s5 to kind-worker2
  Normal  Pulled     13m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    13m   kubelet            Created container kindnet-cni
  Normal  Started    13m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:55:38.540571224Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://262f8c14c3efa06f80c99725c7ef33ccc6df9b2093a71cfbb47686e5770e0e61
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Normal   Pulled     14m                kubelet  Container image "k8s.gcr.io/kube-apiserver:v1.24.7" already present on machine
  Normal   Created    14m                kubelet  Created container kube-apiserver
  Normal   Started    14m                kubelet  Started container kube-apiserver
  Warning  Unhealthy  14m                kubelet  Startup probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  12m (x2 over 13m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500
  Warning  Unhealthy  10s (x2 over 12m)  kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:39 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360892580Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://cf13c24fe12e355e7bc5a1f839c7925749d5cbd801e959e84c4ed12742639e41
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-controller-manager:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-controller-manager
  Normal  Started  14m   kubelet  Started container kube-controller-manager


Name:                 kube-proxy-49wc9
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker2/172.18.0.4
Start Time:           Tue, 19 Sep 2023 23:56:22 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.4
IPs:
  IP:           172.18.0.4
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://977e36f3bedde61f2d3da0a7715667d3975686820d856642a15acf65f0ea00a6
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:33 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4s65b (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-4s65b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned kube-system/kube-proxy-49wc9 to kind-worker2
  Warning  Failed     13m                kubelet            Error: services have not yet been read at least once, cannot construct envvars
  Normal   Pulled     13m (x2 over 13m)  kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal   Created    13m                kubelet            Created container kube-proxy
  Normal   Started    13m                kubelet            Started container kube-proxy


Name:                 kube-proxy-btvpn
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:59 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://43f08fa52ca05a6f125f133ef47c363725de71f17c351847407aa0edad60e9ed
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:05 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lk7z4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lk7z4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-btvpn to kind-control-plane
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-proxy-tkhds
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:56:26 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://d77d7af71678e8216daf48ae112e0ce6ec124215cf2a886121a15d11b50cc4fa
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:40 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c7kq2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-c7kq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  13m   default-scheduler  Successfully assigned kube-system/kube-proxy-tkhds to kind-worker
  Normal  Pulled     13m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    13m   kubelet            Created container kube-proxy
  Normal  Started    13m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:55:38 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:55:10.360902028Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://4c3a0e88f0e4ed0bb445be864541f7714b23d870ef5c31528ce33db47a5868f0
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:55:17 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulled   14m   kubelet  Container image "k8s.gcr.io/kube-scheduler:v1.24.7" already present on machine
  Normal  Created  14m   kubelet  Created container kube-scheduler
  Normal  Started  14m   kubelet  Started container kube-scheduler


Name:                 metrics-server-ffff85765-9kdbh
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:58:43 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=ffff85765
Annotations:          <none>
Status:               Running
IP:                   10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/metrics-server-ffff85765
Containers:
  metrics-server:
    Container ID:  containerd://f285c3cb2d543f87f187c2fcf815276d568d3e6b63e8f2c5cc7e526b16195b5e
    Image:         k8s.gcr.io/metrics-server/metrics-server:v0.6.2
    Image ID:      k8s.gcr.io/metrics-server/metrics-server@sha256:f977ad859fb500c1302d9c3428c6271db031bb7431e7076213b676b345a88dc2
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=10s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:58:52 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8k2qm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-8k2qm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  11m   default-scheduler  Successfully assigned kube-system/metrics-server-ffff85765-9kdbh to kind-worker
  Normal  Pulling    11m   kubelet            Pulling image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2"
  Normal  Pulled     11m   kubelet            Successfully pulled image "k8s.gcr.io/metrics-server/metrics-server:v0.6.2" in 7.372211847s
  Normal  Created    11m   kubelet            Created container metrics-server
  Normal  Started    11m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-prb6b
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:56:09 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://261a04d0ad88808ca368a20c8c459c828e007f95f88623d62a8c8dafb928fb1b
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:56:13 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jbxvx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-jbxvx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  13m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         13m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-prb6b to kind-control-plane
  Normal   Pulled            13m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           13m   kubelet            Created container local-path-provisioner
  Normal   Started           13m   kubelet            Started container local-path-provisioner
