idx,0,LOG_TIME,2023-09-20T04:42:55,2023-09-19T23:42:55
Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-t478n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:04:26 -0500
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Running
IP:                   10.244.1.3
IPs:
  IP:           10.244.1.3
Controlled By:  ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Container ID:  containerd://88f97a374d004f5a36cef51f1f3b6c7b535ed3a2cbd52c66746f90810012dbed
    Image:         registry.k8s.io/descheduler/descheduler:v0.25.1
    Image ID:      registry.k8s.io/descheduler/descheduler@sha256:1d4028fdd33d3eef8b8e5e2450bb3268bb45fa193daa0723b67ac7b8f0a3be72
    Port:          10258/TCP
    Host Port:     0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    State:          Running
      Started:      Tue, 19 Sep 2023 23:04:28 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7hg7z (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-7hg7z:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  38m   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-t478n to kind-worker
  Normal  Pulled     38m   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created    38m   kubelet            Created container descheduler
  Normal  Started    38m   kubelet            Started container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  24m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-m2svv
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:06:57 -0500
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Running
IP:                   10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Container ID:  containerd://a167b34db99c6300fa09395124a339585300fdbccf4799c5b46a0e4db3cbfe02
    Image:         registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Image ID:      registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:06:59 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw5m4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-vw5m4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  35m   default-scheduler  Successfully assigned kube-system/metrics-server-655ddbc78f-m2svv to kind-worker
  Normal  Pulled     35m   kubelet            Container image "registry.k8s.io/metrics-server/metrics-server:v0.6.4" already present on machine
  Normal  Created    35m   kubelet            Created container metrics-server
  Normal  Started    35m   kubelet            Started container metrics-server


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           41m   kubelet            Created container local-path-provisioner
  Normal   Started           41m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         10.244.1.8:4443
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  1s    replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  0s    replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,1,LOG_TIME,2023-09-20T04:42:58,2023-09-19T23:42:58
Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Pod 'descheduler-79b5d46fc5-t478n': error 'pods "descheduler-79b5d46fc5-t478n" not found', but found events.
Events:
  Type    Reason                Age   From               Message
  ----    ------                ----  ----               -------
  Normal  Scheduled             38m   default-scheduler  Successfully assigned kube-system/descheduler-79b5d46fc5-t478n to kind-worker
  Normal  Pulled                38m   kubelet            Container image "registry.k8s.io/descheduler/descheduler:v0.25.1" already present on machine
  Normal  Created               38m   kubelet            Created container descheduler
  Normal  Started               38m   kubelet            Started container descheduler
  Normal  TaintManagerEviction  2s    taint-controller   Marking for deletion Pod kube-system/descheduler-79b5d46fc5-t478n
  Normal  Killing               2s    kubelet            Stopping container descheduler


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  24m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                      metrics-server-655ddbc78f-m2svv
Namespace:                 kube-system
Priority:                  2000000000
Priority Class Name:       system-cluster-critical
Node:                      kind-worker/172.18.0.2
Start Time:                Tue, 19 Sep 2023 23:06:57 -0500
Labels:                    k8s-app=metrics-server
                           pod-template-hash=655ddbc78f
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        10.244.1.8
IPs:
  IP:           10.244.1.8
Controlled By:  ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Container ID:  containerd://a167b34db99c6300fa09395124a339585300fdbccf4799c5b46a0e4db3cbfe02
    Image:         registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Image ID:      registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    State:          Running
      Started:      Tue, 19 Sep 2023 23:06:59 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vw5m4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-vw5m4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  36m   default-scheduler  Successfully assigned kube-system/metrics-server-655ddbc78f-m2svv to kind-worker
  Normal   Pulled     36m   kubelet            Container image "registry.k8s.io/metrics-server/metrics-server:v0.6.4" already present on machine
  Normal   Created    36m   kubelet            Created container metrics-server
  Normal   Started    36m   kubelet            Started container metrics-server
  Normal   Killing    3s    kubelet            Stopping container metrics-server
  Warning  Unhealthy  2s    kubelet            Readiness probe failed: Get "https://10.244.1.8:4443/readyz": dial tcp 10.244.1.8:4443: connect: connection refused


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  2s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           41m   kubelet            Created container local-path-provisioner
  Normal   Started           41m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  3s    replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  2s    replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,2,LOG_TIME,2023-09-20T04:43:00,2023-09-19T23:43:00
Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  4s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  24m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  3s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           41m   kubelet            Created container local-path-provisioner
  Normal   Started           41m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  4s    replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  3s    replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,3,LOG_TIME,2023-09-20T04:43:01,2023-09-19T23:43:01
Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  24m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    42m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  4s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           41m   kubelet            Created container local-path-provisioner
  Normal   Started           41m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  6s    replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  5s    replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,4,LOG_TIME,2023-09-20T04:43:03,2023-09-19T23:43:03
Pod 'php-apache-7f879b889b-c54s5': error 'pods "php-apache-7f879b889b-c54s5" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       2s    kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  2s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-c54s5


Name:                      php-apache-7f879b889b-fn7gt
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Start Time:                Tue, 19 Sep 2023 23:43:03 -0500
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
Reason:                    TaintToleration
Message:                   Pod Predicate TaintToleration failed
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gkczh (ro)
Volumes:
  kube-api-access-gkczh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  1s    kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-p5c7x': error 'pods "php-apache-7f879b889b-p5c7x" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  2s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-p5c7x
  Warning  TaintToleration       2s    kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-qqgc7
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-75ltp (ro)
Volumes:
  kube-api-access-75ltp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  1s    kubelet  Predicate TaintToleration failed


Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           41m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  9s    default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    42m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  24m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  41m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    42m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  10s   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         41m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            41m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           41m   kubelet            Created container local-path-provisioner
  Normal   Started           41m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  41m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   php-apache
Namespace:              default
CreationTimestamp:      Tue, 19 Sep 2023 23:43:02 -0500
Labels:                 run=php-apache
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=php-apache
Replicas:               1 desired | 0 updated | 0 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   php-apache-7f879b889b (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  6s    deployment-controller  Scaled up replica set php-apache-7f879b889b to 1


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  37m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           php-apache-7f879b889b
Namespace:      default
Selector:       pod-template-hash=7f879b889b,run=php-apache
Labels:         pod-template-hash=7f879b889b
                run=php-apache
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/php-apache
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 25 Waiting / 0 Succeeded / 1 Failed
Pod Template:
  Labels:  pod-template-hash=7f879b889b
           run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age               From                   Message
  ----    ------            ----              ----                   -------
  Normal  SuccessfulCreate  7s                replicaset-controller  Created pod: php-apache-7f879b889b-dz2mr
  Normal  SuccessfulCreate  7s                replicaset-controller  Created pod: php-apache-7f879b889b-7g775
  Normal  SuccessfulCreate  7s                replicaset-controller  Created pod: php-apache-7f879b889b-c54s5
  Normal  SuccessfulCreate  7s                replicaset-controller  Created pod: php-apache-7f879b889b-p5c7x
  Normal  SuccessfulCreate  6s                replicaset-controller  Created pod: php-apache-7f879b889b-fn7gt
  Normal  SuccessfulCreate  6s                replicaset-controller  Created pod: php-apache-7f879b889b-qqgc7
  Normal  SuccessfulCreate  6s                replicaset-controller  Created pod: php-apache-7f879b889b-6j8ft
  Normal  SuccessfulCreate  6s                replicaset-controller  Created pod: php-apache-7f879b889b-b66rw
  Normal  SuccessfulCreate  5s                replicaset-controller  Created pod: php-apache-7f879b889b-xd9k4
  Normal  SuccessfulCreate  2s (x16 over 5s)  replicaset-controller  (combined from similar events): Created pod: php-apache-7f879b889b-dx4m4


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  14s   replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  13s   replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  37m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,5,LOG_TIME,2023-09-20T04:43:11,2023-09-19T23:43:11
Name:                      php-apache-7f879b889b-28blp
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mx785 (ro)
Volumes:
  kube-api-access-mx785:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-2lvdv
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-587q7 (ro)
Volumes:
  kube-api-access-587q7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  3s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-4qs7b
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wfxqd (ro)
Volumes:
  kube-api-access-wfxqd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  4s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-4rxtf
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hpxhg (ro)
Volumes:
  kube-api-access-hpxhg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-4rzk9
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4f8j (ro)
Volumes:
  kube-api-access-t4f8j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  5s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-5dnjx
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-79gjl (ro)
Volumes:
  kube-api-access-79gjl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  4s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-5f2hj
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-km9kj (ro)
Volumes:
  kube-api-access-km9kj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-6pj6w
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pd9m8 (ro)
Volumes:
  kube-api-access-pd9m8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  5s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-969jq
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rhxq2 (ro)
Volumes:
  kube-api-access-rhxq2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  7s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-bppxf
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cvdnj (ro)
Volumes:
  kube-api-access-cvdnj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  3s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-cckdw
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wctwt (ro)
Volumes:
  kube-api-access-wctwt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  6s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-cxqng
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-s2688 (ro)
Volumes:
  kube-api-access-s2688:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  3s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-d2ckb
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w45mx (ro)
Volumes:
  kube-api-access-w45mx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  10s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-d7fgv
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wfp5f (ro)
Volumes:
  kube-api-access-wfp5f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-drqs5
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jqbt8 (ro)
Volumes:
  kube-api-access-jqbt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  4s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-dx4m4
Namespace:                 default
Priority:                  0
Node:                      kind-worker/172.18.0.2
Start Time:                Tue, 19 Sep 2023 23:43:07 -0500
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
Reason:                    TaintToleration
Message:                   Pod Predicate TaintToleration failed
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Container ID:   
    Image:          registry.k8s.io/hpa-example
    Image ID:       
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x6x64 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-x6x64:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-h9s9b
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hplbg (ro)
Volumes:
  kube-api-access-hplbg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  5s    kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-j7xmp': error 'pods "php-apache-7f879b889b-j7xmp" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       9s    kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  9s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-j7xmp


Pod 'php-apache-7f879b889b-jxltt': error 'pods "php-apache-7f879b889b-jxltt" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       9s    kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  9s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-jxltt


Pod 'php-apache-7f879b889b-m6k2c': error 'pods "php-apache-7f879b889b-m6k2c" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       9s    kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  9s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-m6k2c


Pod 'php-apache-7f879b889b-mt2vn': error 'pods "php-apache-7f879b889b-mt2vn" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  9s    taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-mt2vn
  Warning  TaintToleration       9s    kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-ng796
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g5rcq (ro)
Volumes:
  kube-api-access-g5rcq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  6s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-p6578
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ggfp5 (ro)
Volumes:
  kube-api-access-ggfp5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  12s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-pcckc
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zl85q (ro)
Volumes:
  kube-api-access-zl85q:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  7s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-r2xg7
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tcd8l (ro)
Volumes:
  kube-api-access-tcd8l:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  7s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-sk8dd
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6qf2 (ro)
Volumes:
  kube-api-access-p6qf2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  6s    kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-wkcf9': error 'pods "php-apache-7f879b889b-wkcf9" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       10s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  10s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-wkcf9


Pod 'php-apache-7f879b889b-xd9k4': error 'pods "php-apache-7f879b889b-xd9k4" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  13s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-xd9k4
  Warning  TaintToleration       13s   kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-xdpml
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5d7sv (ro)
Volumes:
  kube-api-access-5d7sv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-xkpwq
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kjwj5 (ro)
Volumes:
  kube-api-access-kjwj5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-zh5bn
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-77jj5 (ro)
Volumes:
  kube-api-access-77jj5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-zhqrj
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z9kvn (ro)
Volumes:
  kube-api-access-z9kvn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           42m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           42m   kubelet            Created container coredns
  Normal   Started           41m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  22s   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    42m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     41m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    41m   kubelet            Created container kindnet-cni
  Normal  Started    41m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  25m (x2 over 41m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     41m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    41m   kubelet            Created container kube-proxy
  Normal  Started    41m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    42m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           42m   kubelet            Created container local-path-provisioner
  Normal   Started           42m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   php-apache
Namespace:              default
CreationTimestamp:      Tue, 19 Sep 2023 23:43:02 -0500
Labels:                 run=php-apache
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=php-apache
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   php-apache-7f879b889b (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  20s   deployment-controller  Scaled up replica set php-apache-7f879b889b to 1


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  39m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  35m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           php-apache-7f879b889b
Namespace:      default
Selector:       pod-template-hash=7f879b889b,run=php-apache
Labels:         pod-template-hash=7f879b889b
                run=php-apache
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/php-apache
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 70 Waiting / 0 Succeeded / 1 Failed
Pod Template:
  Labels:  pod-template-hash=7f879b889b
           run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age                 From                   Message
  ----    ------            ----                ----                   -------
  Normal  SuccessfulCreate  21s                 replicaset-controller  Created pod: php-apache-7f879b889b-dz2mr
  Normal  SuccessfulCreate  21s                 replicaset-controller  Created pod: php-apache-7f879b889b-7g775
  Normal  SuccessfulCreate  21s                 replicaset-controller  Created pod: php-apache-7f879b889b-c54s5
  Normal  SuccessfulCreate  21s                 replicaset-controller  Created pod: php-apache-7f879b889b-p5c7x
  Normal  SuccessfulCreate  20s                 replicaset-controller  Created pod: php-apache-7f879b889b-fn7gt
  Normal  SuccessfulCreate  20s                 replicaset-controller  Created pod: php-apache-7f879b889b-qqgc7
  Normal  SuccessfulCreate  20s                 replicaset-controller  Created pod: php-apache-7f879b889b-6j8ft
  Normal  SuccessfulCreate  20s                 replicaset-controller  Created pod: php-apache-7f879b889b-b66rw
  Normal  SuccessfulCreate  19s                 replicaset-controller  Created pod: php-apache-7f879b889b-xd9k4
  Normal  SuccessfulCreate  16s (x16 over 19s)  replicaset-controller  (combined from similar events): Created pod: php-apache-7f879b889b-dx4m4


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  38m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  28s   replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  27s   replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  35m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
idx,6,LOG_TIME,2023-09-20T04:43:25,2023-09-19T23:43:25
Name:                      php-apache-7f879b889b-25gqk
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8qjkt (ro)
Volumes:
  kube-api-access-8qjkt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  6s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-2lvdv
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-587q7 (ro)
Volumes:
  kube-api-access-587q7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  17s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-2vvfz
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8lffb (ro)
Volumes:
  kube-api-access-8lffb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-2w8c7
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tfv9h (ro)
Volumes:
  kube-api-access-tfv9h:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  2s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-45pj8
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rlbkf (ro)
Volumes:
  kube-api-access-rlbkf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-4bprh
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w2lrz (ro)
Volumes:
  kube-api-access-w2lrz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-4rzk9
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4f8j (ro)
Volumes:
  kube-api-access-t4f8j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  20s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-5dnjx
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-79gjl (ro)
Volumes:
  kube-api-access-79gjl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  19s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-6bqwn
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8rzrh (ro)
Volumes:
  kube-api-access-8rzrh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  7s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-6hrd5
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t98js (ro)
Volumes:
  kube-api-access-t98js:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  16s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-6sfs6': error 'pods "php-apache-7f879b889b-6sfs6" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       11s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  11s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-6sfs6


Name:                      php-apache-7f879b889b-7rkpn
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-78fqm (ro)
Volumes:
  kube-api-access-78fqm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8cgwx
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-82567 (ro)
Volumes:
  kube-api-access-82567:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  11s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8dz4m
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ztr8b (ro)
Volumes:
  kube-api-access-ztr8b:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  8s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8fj5s
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l7rdv (ro)
Volumes:
  kube-api-access-l7rdv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  5s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8gzh5
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ffcbn (ro)
Volumes:
  kube-api-access-ffcbn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  17s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8hd5x
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n5x2h (ro)
Volumes:
  kube-api-access-n5x2h:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  19s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-8wwzv
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gh98s (ro)
Volumes:
  kube-api-access-gh98s:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  16s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-94qmx
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xtzrq (ro)
Volumes:
  kube-api-access-xtzrq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  11s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-9sgts
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dpq42 (ro)
Volumes:
  kube-api-access-dpq42:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  16s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-9wtrc
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwk4p (ro)
Volumes:
  kube-api-access-rwk4p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-b556h
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hwzcm (ro)
Volumes:
  kube-api-access-hwzcm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  7s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-c98d7
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rfg9x (ro)
Volumes:
  kube-api-access-rfg9x:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  19s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-cckdw
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wctwt (ro)
Volumes:
  kube-api-access-wctwt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  24s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-cj9fl
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-67bj4 (ro)
Volumes:
  kube-api-access-67bj4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-cxqng': error 'pods "php-apache-7f879b889b-cxqng" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       21s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  21s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-cxqng


Name:                      php-apache-7f879b889b-d4m7s
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-djjww (ro)
Volumes:
  kube-api-access-djjww:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  9s    kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-d6l2x
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bzhgb (ro)
Volumes:
  kube-api-access-bzhgb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  16s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-drqs5': error 'pods "php-apache-7f879b889b-drqs5" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  21s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-drqs5
  Warning  TaintToleration       21s   kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-fbsrq
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9285z (ro)
Volumes:
  kube-api-access-9285z:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  20s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-fn5h5
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d2k9v (ro)
Volumes:
  kube-api-access-d2k9v:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  18s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-ftqvc
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wtzns (ro)
Volumes:
  kube-api-access-wtzns:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  15s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-fw75m
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t5f8d (ro)
Volumes:
  kube-api-access-t5f8d:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  11s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-h9s9b
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hplbg (ro)
Volumes:
  kube-api-access-hplbg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  23s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-hjmfr
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-44nbf (ro)
Volumes:
  kube-api-access-44nbf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  10s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-hm8rg
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lw5pb (ro)
Volumes:
  kube-api-access-lw5pb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  10s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-jm4rp
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sjqv (ro)
Volumes:
  kube-api-access-7sjqv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  10s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-jn6s9
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5752x (ro)
Volumes:
  kube-api-access-5752x:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  21s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-jvvsh
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5g85m (ro)
Volumes:
  kube-api-access-5g85m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  15s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-km9p4': error 'pods "php-apache-7f879b889b-km9p4" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       20s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  20s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-km9p4


Name:                      php-apache-7f879b889b-kshfv
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9mlp6 (ro)
Volumes:
  kube-api-access-9mlp6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  14s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-kwwq8
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-crzr2 (ro)
Volumes:
  kube-api-access-crzr2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  21s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-kz2s6': error 'pods "php-apache-7f879b889b-kz2s6" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  19s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-kz2s6
  Warning  TaintToleration       19s   kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-l8gfr
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nn8f2 (ro)
Volumes:
  kube-api-access-nn8f2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  20s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-lqd85
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6r9j (ro)
Volumes:
  kube-api-access-p6r9j:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-mt77p
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-92r9s (ro)
Volumes:
  kube-api-access-92r9s:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-n9zzs
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lgrdh (ro)
Volumes:
  kube-api-access-lgrdh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  17s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-ng796': error 'pods "php-apache-7f879b889b-ng796" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       26s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  26s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-ng796


Pod 'php-apache-7f879b889b-nrdzm': error 'pods "php-apache-7f879b889b-nrdzm" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       22s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  22s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-nrdzm


Pod 'php-apache-7f879b889b-p9krm': error 'pods "php-apache-7f879b889b-p9krm" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  20s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-p9krm
  Warning  TaintToleration       20s   kubelet           Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-pcckc': error 'pods "php-apache-7f879b889b-pcckc" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       28s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  28s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-pcckc


Name:                      php-apache-7f879b889b-pxpv6
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cpz6m (ro)
Volumes:
  kube-api-access-cpz6m:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  17s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-pzf6p
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zpnl2 (ro)
Volumes:
  kube-api-access-zpnl2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  15s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-qn4cl
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rwz94 (ro)
Volumes:
  kube-api-access-rwz94:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-qx2sf
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7ljk8 (ro)
Volumes:
  kube-api-access-7ljk8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  13s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-sk8dd
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p6qf2 (ro)
Volumes:
  kube-api-access-p6qf2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  27s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-sq9zd
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-khzt9 (ro)
Volumes:
  kube-api-access-khzt9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  19s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-svxsm
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-nrg99 (ro)
Volumes:
  kube-api-access-nrg99:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  24s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-t6c59
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qkns4 (ro)
Volumes:
  kube-api-access-qkns4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  21s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-tgkxr
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9q9lw (ro)
Volumes:
  kube-api-access-9q9lw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  26s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-tnx5m
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5bz4g (ro)
Volumes:
  kube-api-access-5bz4g:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  26s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-tq2ql
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rfx47 (ro)
Volumes:
  kube-api-access-rfx47:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  18s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-vk8fl
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4zdgp (ro)
Volumes:
  kube-api-access-4zdgp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  17s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-wck9x
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2mkb7 (ro)
Volumes:
  kube-api-access-2mkb7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  27s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-wn875
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xn5mb (ro)
Volumes:
  kube-api-access-xn5mb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  18s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-wt66p': error 'pods "php-apache-7f879b889b-wt66p" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       28s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  28s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-wt66p


Name:                      php-apache-7f879b889b-wtnf9
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-txgj7 (ro)
Volumes:
  kube-api-access-txgj7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  19s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-x2zpd': error 'pods "php-apache-7f879b889b-x2zpd" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       28s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  28s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-x2zpd


Name:                      php-apache-7f879b889b-x52cn
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rqbl6 (ro)
Volumes:
  kube-api-access-rqbl6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  23s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-xcdhw
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7cfgp (ro)
Volumes:
  kube-api-access-7cfgp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  15s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-xdpml
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts 2s)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5d7sv (ro)
Volumes:
  kube-api-access-5d7sv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  32s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-xkpwq': error 'pods "php-apache-7f879b889b-xkpwq" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  31s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-xkpwq
  Warning  TaintToleration       31s   kubelet           Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-xxr5r': error 'pods "php-apache-7f879b889b-xxr5r" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       25s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  25s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-xxr5r


Name:                      php-apache-7f879b889b-z4zvr
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5gh48 (ro)
Volumes:
  kube-api-access-5gh48:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  18s   kubelet  Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-zh5bn
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts 1s)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-77jj5 (ro)
Volumes:
  kube-api-access-77jj5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  31s   kubelet  Predicate TaintToleration failed


Pod 'php-apache-7f879b889b-zhqrj': error 'pods "php-apache-7f879b889b-zhqrj" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Warning  TaintToleration       31s   kubelet           Predicate TaintToleration failed
  Normal   TaintManagerEviction  31s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-zhqrj


Pod 'php-apache-7f879b889b-zxtcz': error 'pods "php-apache-7f879b889b-zxtcz" not found', but found events.
Events:
  Type     Reason                Age   From              Message
  ----     ------                ----  ----              -------
  Normal   TaintManagerEviction  30s   taint-controller  Marking for deletion Pod default/php-apache-7f879b889b-zxtcz
  Warning  TaintToleration       30s   kubelet           Predicate TaintToleration failed


Name:                      php-apache-7f879b889b-zz2hk
Namespace:                 default
Priority:                  0
Node:                      kind-worker/
Labels:                    pod-template-hash=7f879b889b
                           run=php-apache
Annotations:               <none>
Status:                    Terminating (lasts <invalid>)
Termination Grace Period:  30s
IP:                        
IPs:                       <none>
Controlled By:             ReplicaSet/php-apache-7f879b889b
Containers:
  php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8nglr (ro)
Volumes:
  kube-api-access-8nglr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason           Age   From     Message
  ----     ------           ----  ----     -------
  Warning  TaintToleration  18s   kubelet  Predicate TaintToleration failed


Name:                 coredns-6d4b75cb6d-qt4fn
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ed0fccc4bdc82839af5d72045e5ae6efd00786d0da09af3cc970d55443d074cb
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bhvt8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-bhvt8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-qt4fn to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           42m   kubelet            Created container coredns
  Normal   Started           42m   kubelet            Started container coredns


Name:                 coredns-6d4b75cb6d-ztd7n
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:01:10 -0500
Labels:               k8s-app=kube-dns
                      pod-template-hash=6d4b75cb6d
Annotations:          <none>
Status:               Running
IP:                   10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-6d4b75cb6d
Containers:
  coredns:
    Container ID:  containerd://ab20a2fcf508f862cbce35a35d84ff6641781701a2806529a8170fd7ecc73299
    Image:         k8s.gcr.io/coredns/coredns:v1.8.6
    Image ID:      sha256:a4ca41631cc7ac19ce1be3ebf0314ac5f47af7c711f17066006db82ee3b75b03
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vxn27 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-vxn27:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 CriticalAddonsOnly op=Exists
                             node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned kube-system/coredns-6d4b75cb6d-ztd7n to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "k8s.gcr.io/coredns/coredns:v1.8.6" already present on machine
  Normal   Created           42m   kubelet            Created container coredns
  Normal   Started           42m   kubelet            Started container coredns


Name:                 descheduler-79b5d46fc5-sp84r
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               app=descheduler
                      pod-template-hash=79b5d46fc5
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/descheduler-79b5d46fc5
Containers:
  descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kf9b4 (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  policy-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      descheduler-policy-configmap
    Optional:  false
  kube-api-access-kf9b4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  45s   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:                 etcd-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:39 -0500
Labels:               component=etcd
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.18.0.3:2379
                      kubernetes.io/config.hash: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.mirror: 87881725128f7faa2a83d7821f02d1ef
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284233305Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  etcd:
    Container ID:  containerd://1bd63ee24964fd65543acb81035ad552a9b817d88cb31829f08f367fbfa256c5
    Image:         k8s.gcr.io/etcd:3.5.3-0
    Image ID:      sha256:aebe758cef4cd05b9f8cee39758227714d02f42ef3088023c1e3cd454f927a2b
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://172.18.0.3:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --experimental-initial-corrupt-check=true
      --initial-advertise-peer-urls=https://172.18.0.3:2380
      --initial-cluster=kind-control-plane=https://172.18.0.3:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://172.18.0.3:2379
      --listen-metrics-urls=http://127.0.0.1:2381
      --listen-peer-urls=https://172.18.0.3:2380
      --name=kind-control-plane
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:25 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
      memory:     100Mi
    Liveness:     http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get http://127.0.0.1:2381/health delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:         kindnet-gqgr6
Namespace:    kube-system
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:00:51 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://18cedec5cf505aa69303191289a86eec873835447a3f8aa0fb8e30262cfc6810
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:02 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdsw6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-jdsw6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-gqgr6 to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    42m   kubelet            Started container kindnet-cni


Name:         kindnet-p2499
Namespace:    kube-system
Priority:     0
Node:         kind-worker/172.18.0.2
Start Time:   Tue, 19 Sep 2023 23:01:18 -0500
Labels:       app=kindnet
              controller-revision-hash=77ddccf744
              k8s-app=kindnet
              pod-template-generation=1
              tier=node
Annotations:  <none>
Status:       Running
IP:           172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kindnet
Containers:
  kindnet-cni:
    Container ID:   containerd://52a9374e1577f55a1563d98f2241490ad190c21daf8a1b1ade790e8557dadc1c
    Image:          docker.io/kindest/kindnetd:v20221004-44d545d1
    Image ID:       sha256:d6e3e26021b60c625f0ef5b2dd3f9e22d2d398e05bccc4fdd7d59fbbb6a04d3f
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:32 -0500
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lktnq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-lktnq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kindnet-p2499 to kind-worker
  Normal  Pulled     42m   kubelet            Container image "docker.io/kindest/kindnetd:v20221004-44d545d1" already present on machine
  Normal  Created    42m   kubelet            Created container kindnet-cni
  Normal  Started    42m   kubelet            Started container kindnet-cni


Name:                 kube-apiserver-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 172.18.0.3:6443
                      kubernetes.io/config.hash: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.mirror: f32f0d703298ac153a0438ea4aff2115
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284241123Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-apiserver:
    Container ID:  containerd://9ef5af3df892c00be57c9880528f77899b84769384724daf5d4a83372351fddd
    Image:         k8s.gcr.io/kube-apiserver:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:28b84228b7ad7ce125a759a1a72b78c5f8ff2130596b4c8d4d3920795a8c1c02
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=172.18.0.3
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --runtime-config=
      --secure-port=6443
      --service-account-issuer=https://kubernetes.default.svc.cluster.local
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:20 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=8
    Readiness:    http-get https://172.18.0.3:6443/readyz delay=0s timeout=15s period=1s #success=1 #failure=3
    Startup:      http-get https://172.18.0.3:6443/livez delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason     Age                From     Message
  ----     ------     ----               ----     -------
  Warning  Unhealthy  25m (x2 over 42m)  kubelet  Readiness probe failed: HTTP probe failed with statuscode: 500


Name:                 kube-controller-manager-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:13 -0500
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.mirror: 4faf275681440680a008f7e131ea87f4
                      kubernetes.io/config.seen: 2023-09-20T04:00:12.370452735Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-controller-manager:
    Container ID:  containerd://b1a40d68e912d7ce78d296d5b1c8f1e03226bfa06d0ee5450b14ce79daad6213
    Image:         k8s.gcr.io/kube-controller-manager:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:1fb7081748a633622f6da6bd314e4c83ffd3f83864943cfc9afe72f6371b54a4
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-name=kind
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --enable-hostpath-provisioner=true
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=10.96.0.0/16
      --use-service-account-credentials=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 kube-proxy-9k4lv
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-worker/172.18.0.2
Start Time:           Tue, 19 Sep 2023 23:01:18 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.2
IPs:
  IP:           172.18.0.2
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://e2db84c2e3581fb4c833f9309724dcc9560547b21c525f0a3e9e9b6cce14f4f4
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:26 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mr2s9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mr2s9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-9k4lv to kind-worker
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    42m   kubelet            Started container kube-proxy


Name:                 kube-proxy-kwmbs
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:51 -0500
Labels:               controller-revision-hash=547546f9bf
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://da12a99330611ef290bb90594fac7dd2ca9e02c2a5f1e6e6e745bffb0ad57598
    Image:         k8s.gcr.io/kube-proxy:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:82fa4ddf64d5be44671118fb9b86c454a222ff7ea6e48863ed7e5daeeca3ea42
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:01 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5wvl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-f5wvl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42m   default-scheduler  Successfully assigned kube-system/kube-proxy-kwmbs to kind-control-plane
  Normal  Pulled     42m   kubelet            Container image "k8s.gcr.io/kube-proxy:v1.24.7" already present on machine
  Normal  Created    42m   kubelet            Created container kube-proxy
  Normal  Started    42m   kubelet            Started container kube-proxy


Name:                 kube-scheduler-kind-control-plane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 kind-control-plane/172.18.0.3
Start Time:           Tue, 19 Sep 2023 23:00:40 -0500
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.mirror: 3fde76549d954b91a2380b5221f56cd5
                      kubernetes.io/config.seen: 2023-09-20T04:00:39.284244785Z
                      kubernetes.io/config.source: file
                      seccomp.security.alpha.kubernetes.io/pod: runtime/default
Status:               Running
IP:                   172.18.0.3
IPs:
  IP:           172.18.0.3
Controlled By:  Node/kind-control-plane
Containers:
  kube-scheduler:
    Container ID:  containerd://bd6eb269f8129848c181d1bdf9d0c4ded57f2b46ce721aa13f8f7c78fff4044d
    Image:         k8s.gcr.io/kube-scheduler:v1.24.7
    Image ID:      docker.io/library/import-2022-10-26@sha256:fd99c23a5f64fec4b55db13d4d7b87f68ef17c39d85242fcbaa305cf386f6ecc
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Tue, 19 Sep 2023 23:00:19 -0500
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:            <none>


Name:                 metrics-server-655ddbc78f-r7ws5
Namespace:            kube-system
Priority:             2000000000
Priority Class Name:  system-cluster-critical
Node:                 <none>
Labels:               k8s-app=metrics-server
                      pod-template-hash=655ddbc78f
Annotations:          <none>
Status:               Pending
IP:                   
IPs:                  <none>
Controlled By:        ReplicaSet/metrics-server-655ddbc78f
Containers:
  metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jjlrn (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  tmp-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kube-api-access-jjlrn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  45s   default-scheduler  0/2 nodes are available: 1 node(s) had untolerated taint {key1: value1}, 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.


Name:         local-path-provisioner-6b84c5c67f-rt2ph
Namespace:    local-path-storage
Priority:     0
Node:         kind-control-plane/172.18.0.3
Start Time:   Tue, 19 Sep 2023 23:01:10 -0500
Labels:       app=local-path-provisioner
              pod-template-hash=6b84c5c67f
Annotations:  <none>
Status:       Running
IP:           10.244.0.4
IPs:
  IP:           10.244.0.4
Controlled By:  ReplicaSet/local-path-provisioner-6b84c5c67f
Containers:
  local-path-provisioner:
    Container ID:  containerd://49ca79d3e8f832736af89ddd30a94d916e9bf050dee7985189f8378f87fe60fb
    Image:         docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Image ID:      sha256:4c1e997385b8fb4ad4d1d3c7e5af7ff3f882e94d07cf5b78de9e889bc60830e6
    Port:          <none>
    Host Port:     <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    State:          Running
      Started:      Tue, 19 Sep 2023 23:01:19 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      POD_NAMESPACE:  local-path-storage (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gvh45 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
  kube-api-access-gvh45:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule
                             node-role.kubernetes.io/master:NoSchedule
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  42m   default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
  Normal   Scheduled         42m   default-scheduler  Successfully assigned local-path-storage/local-path-provisioner-6b84c5c67f-rt2ph to kind-control-plane
  Normal   Pulled            42m   kubelet            Container image "docker.io/kindest/local-path-provisioner:v0.0.22-kind.0" already present on machine
  Normal   Created           42m   kubelet            Created container local-path-provisioner
  Normal   Started           42m   kubelet            Started container local-path-provisioner


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.18.0.3:6443
Session Affinity:  None
Events:            <none>


Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            <none>


Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.50.225
IPs:               10.96.50.225
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         <none>
Session Affinity:  None
Events:            <none>


Name:           kindnet
Selector:       app=kindnet
Node-Selector:  <none>
Labels:         app=kindnet
                k8s-app=kindnet
                tier=node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=kindnet
                    k8s-app=kindnet
                    tier=node
  Service Account:  kindnet
  Containers:
   kindnet-cni:
    Image:      docker.io/kindest/kindnetd:v20221004-44d545d1
    Port:       <none>
    Host Port:  <none>
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      HOST_IP:                  (v1:status.hostIP)
      POD_IP:                   (v1:status.podIP)
      POD_SUBNET:              10.244.0.0/16
      CONTROL_PLANE_ENDPOINT:  kind-control-plane:6443
    Mounts:
      /etc/cni/net.d from cni-cfg (rw)
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
  Volumes:
   cni-cfg:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-gqgr6
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kindnet-p2499


Name:           kube-proxy
Selector:       k8s-app=kube-proxy
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=kube-proxy
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-proxy
  Service Account:  kube-proxy
  Containers:
   kube-proxy:
    Image:      k8s.gcr.io/kube-proxy:v1.24.7
    Port:       <none>
    Host Port:  <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
  Volumes:
   kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
   xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
   lib-modules:
    Type:               HostPath (bare host directory volume)
    Path:               /lib/modules
    HostPathType:       
  Priority Class Name:  system-node-critical
Events:
  Type    Reason            Age   From                  Message
  ----    ------            ----  ----                  -------
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-kwmbs
  Normal  SuccessfulCreate  42m   daemonset-controller  Created pod: kube-proxy-9k4lv


Name:                   php-apache
Namespace:              default
CreationTimestamp:      Tue, 19 Sep 2023 23:43:02 -0500
Labels:                 run=php-apache
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               run=php-apache
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   php-apache-7f879b889b (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42s   deployment-controller  Scaled up replica set php-apache-7f879b889b to 1


Name:                   coredns
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:00:39 -0500
Labels:                 k8s-app=kube-dns
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               k8s-app=kube-dns
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=kube-dns
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   coredns-6d4b75cb6d (2/2 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set coredns-6d4b75cb6d to 2


Name:                   descheduler
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:04:14 -0500
Labels:                 app=descheduler
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=descheduler
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=descheduler
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   descheduler-79b5d46fc5 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  39m   deployment-controller  Scaled up replica set descheduler-6bbbff857b to 1
  Normal  ScalingReplicaSet  39m   deployment-controller  Scaled up replica set descheduler-79b5d46fc5 to 1
  Normal  ScalingReplicaSet  39m   deployment-controller  Scaled down replica set descheduler-6bbbff857b to 0


Name:                   metrics-server
Namespace:              kube-system
CreationTimestamp:      Tue, 19 Sep 2023 23:05:19 -0500
Labels:                 k8s-app=metrics-server
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               k8s-app=metrics-server
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  0 max unavailable, 25% max surge
Pod Template:
  Labels:           k8s-app=metrics-server
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  <none>
NewReplicaSet:   metrics-server-655ddbc78f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  38m   deployment-controller  Scaled up replica set metrics-server-8cc45cd8d to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled up replica set metrics-server-655ddbc78f to 1
  Normal  ScalingReplicaSet  36m   deployment-controller  Scaled down replica set metrics-server-8cc45cd8d to 0


Name:                   local-path-provisioner
Namespace:              local-path-storage
CreationTimestamp:      Tue, 19 Sep 2023 23:00:48 -0500
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=local-path-provisioner
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=local-path-provisioner
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   local-path-provisioner-6b84c5c67f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  42m   deployment-controller  Scaled up replica set local-path-provisioner-6b84c5c67f to 1


Name:           php-apache-7f879b889b
Namespace:      default
Selector:       pod-template-hash=7f879b889b,run=php-apache
Labels:         pod-template-hash=7f879b889b
                run=php-apache
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/php-apache
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 138 Waiting / 0 Succeeded / 1 Failed
Pod Template:
  Labels:  pod-template-hash=7f879b889b
           run=php-apache
  Containers:
   php-apache:
    Image:        registry.k8s.io/hpa-example
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age                 From                   Message
  ----    ------            ----                ----                   -------
  Normal  SuccessfulCreate  43s                 replicaset-controller  Created pod: php-apache-7f879b889b-dz2mr
  Normal  SuccessfulCreate  43s                 replicaset-controller  Created pod: php-apache-7f879b889b-7g775
  Normal  SuccessfulCreate  43s                 replicaset-controller  Created pod: php-apache-7f879b889b-c54s5
  Normal  SuccessfulCreate  43s                 replicaset-controller  Created pod: php-apache-7f879b889b-p5c7x
  Normal  SuccessfulCreate  42s                 replicaset-controller  Created pod: php-apache-7f879b889b-fn7gt
  Normal  SuccessfulCreate  42s                 replicaset-controller  Created pod: php-apache-7f879b889b-qqgc7
  Normal  SuccessfulCreate  42s                 replicaset-controller  Created pod: php-apache-7f879b889b-6j8ft
  Normal  SuccessfulCreate  42s                 replicaset-controller  Created pod: php-apache-7f879b889b-b66rw
  Normal  SuccessfulCreate  41s                 replicaset-controller  Created pod: php-apache-7f879b889b-xd9k4
  Normal  SuccessfulCreate  38s (x16 over 41s)  replicaset-controller  (combined from similar events): Created pod: php-apache-7f879b889b-dx4m4


Name:           coredns-6d4b75cb6d
Namespace:      kube-system
Selector:       k8s-app=kube-dns,pod-template-hash=6d4b75cb6d
Labels:         k8s-app=kube-dns
                pod-template-hash=6d4b75cb6d
Annotations:    deployment.kubernetes.io/desired-replicas: 2
                deployment.kubernetes.io/max-replicas: 3
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/coredns
Replicas:       2 current / 2 desired
Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=kube-dns
                    pod-template-hash=6d4b75cb6d
  Service Account:  coredns
  Containers:
   coredns:
    Image:       k8s.gcr.io/coredns/coredns:v1.8.6
    Ports:       53/UDP, 53/TCP, 9153/TCP
    Host Ports:  0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
  Volumes:
   config-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               coredns
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-ztd7n
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: coredns-6d4b75cb6d-qt4fn


Name:           descheduler-6bbbff857b
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=6bbbff857b
Labels:         app=descheduler
                pod-template-hash=6bbbff857b
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/descheduler
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=6bbbff857b
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      5m
      --v
      3
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39m   replicaset-controller  Created pod: descheduler-6bbbff857b-pbqd4
  Normal  SuccessfulDelete  39m   replicaset-controller  Deleted pod: descheduler-6bbbff857b-pbqd4


Name:           descheduler-79b5d46fc5
Namespace:      kube-system
Selector:       app=descheduler,pod-template-hash=79b5d46fc5
Labels:         app=descheduler
                pod-template-hash=79b5d46fc5
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/descheduler
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=descheduler
                    pod-template-hash=79b5d46fc5
  Service Account:  descheduler-sa
  Containers:
   descheduler:
    Image:      registry.k8s.io/descheduler/descheduler:v0.25.1
    Port:       10258/TCP
    Host Port:  0/TCP
    Command:
      /bin/descheduler
    Args:
      --policy-config-file
      /policy-dir/policy.yaml
      --descheduling-interval
      10s
      --v
      4
    Requests:
      cpu:        500m
      memory:     256Mi
    Liveness:     http-get https://:10258/healthz delay=3s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /policy-dir from policy-volume (rw)
  Volumes:
   policy-volume:
    Type:               ConfigMap (a volume populated by a ConfigMap)
    Name:               descheduler-policy-configmap
    Optional:           false
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  39m   replicaset-controller  Created pod: descheduler-79b5d46fc5-t478n
  Normal  SuccessfulCreate  50s   replicaset-controller  Created pod: descheduler-79b5d46fc5-sp84r


Name:           metrics-server-655ddbc78f
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=655ddbc78f
Labels:         k8s-app=metrics-server
                pod-template-hash=655ddbc78f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 2
Controlled By:  Deployment/metrics-server
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=655ddbc78f
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --kubelet-insecure-tls
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  36m   replicaset-controller  Created pod: metrics-server-655ddbc78f-m2svv
  Normal  SuccessfulCreate  49s   replicaset-controller  Created pod: metrics-server-655ddbc78f-r7ws5


Name:           metrics-server-8cc45cd8d
Namespace:      kube-system
Selector:       k8s-app=metrics-server,pod-template-hash=8cc45cd8d
Labels:         k8s-app=metrics-server
                pod-template-hash=8cc45cd8d
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/metrics-server
Replicas:       0 current / 0 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=metrics-server
                    pod-template-hash=8cc45cd8d
  Service Account:  metrics-server
  Containers:
   metrics-server:
    Image:      registry.k8s.io/metrics-server/metrics-server:v0.6.4
    Port:       4443/TCP
    Host Port:  0/TCP
    Args:
      --cert-dir=/tmp
      --secure-port=4443
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --kubelet-use-node-status-port
      --metric-resolution=15s
    Requests:
      cpu:        100m
      memory:     200Mi
    Liveness:     http-get https://:https/livez delay=0s timeout=1s period=10s #success=1 #failure=3
    Readiness:    http-get https://:https/readyz delay=20s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /tmp from tmp-dir (rw)
  Volumes:
   tmp-dir:
    Type:               EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:             
    SizeLimit:          <unset>
  Priority Class Name:  system-cluster-critical
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  38m   replicaset-controller  Created pod: metrics-server-8cc45cd8d-s8cxz
  Normal  SuccessfulDelete  36m   replicaset-controller  Deleted pod: metrics-server-8cc45cd8d-s8cxz


Name:           local-path-provisioner-6b84c5c67f
Namespace:      local-path-storage
Selector:       app=local-path-provisioner,pod-template-hash=6b84c5c67f
Labels:         app=local-path-provisioner
                pod-template-hash=6b84c5c67f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/local-path-provisioner
Replicas:       1 current / 1 desired
Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           app=local-path-provisioner
                    pod-template-hash=6b84c5c67f
  Service Account:  local-path-provisioner-service-account
  Containers:
   local-path-provisioner:
    Image:      docker.io/kindest/local-path-provisioner:v0.0.22-kind.0
    Port:       <none>
    Host Port:  <none>
    Command:
      local-path-provisioner
      --debug
      start
      --helper-image
      docker.io/kindest/local-path-helper:v20220607-9a4d8d2a
      --config
      /etc/config/config.json
    Environment:
      POD_NAMESPACE:   (v1:metadata.namespace)
    Mounts:
      /etc/config/ from config-volume (rw)
  Volumes:
   config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      local-path-config
    Optional:  false
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  42m   replicaset-controller  Created pod: local-path-provisioner-6b84c5c67f-rt2ph
